% !Mode:: "TeX:UTF-8" 

\BiChapter{研究可编程设备加速主机侧网络方法}{NIC}


\BiSection{本章引论}{}

\BiSection{问题背景}{aa}

%有什么东西软件做不好的，
%
%为什么要选流量工程 和 压缩加速。一定要好好想明白。 每包计算是最费劲儿的。
%流量系统背景和测量背景， %1
%
%软件流量捕捉 回放能力，

随着数据中心服务器网络接口容量快速增长，处理与网络数据相关的服务已越来越耗费主机CPU的计算资源。主机虚拟化、流量工程、网络监管等功能在现代化网络管理中已经占据重要的位置。尽管目前发展出一系列RDMA、DPDK等基于网卡的网络数据包快速搬运架构，但它们只针对于块儿数据的传递进行了优化，对于需要“每包处理”的任务依然没有良好的对策。



本文将复杂网络计算问题分为两类任务类型，一种类型是由于数据包到达频繁而触发的大量计算，也许这类型计算并不复杂（处理器使用几条指令即可完成计算），但由于需要对每个数据包都进行处理从而导致计算量庞大、CPU无法胜任，例如查找、转发、分类等。第二种类型是由于计算过程复杂（处理器需要耗费多条指令才可完成计算），导致CPU无法提升针对每个数据包处理的速度，进而造成网络分发数据包个数降低、性能需求无法胜任，例如防火墙、安全分析等。数据中心运营商面对着不断增长的功能需求和性能需求，同时也面对着需要降低运营成本提升能源利用率和绿色环保。

网络随路计算（in-network computing）是一种解决网络程序性能差的有效途径。网络随路计算是指在网络数据传输链路中增加特定功能的硬件设备，使得数据包在传送到主机内部之前，就完成了网络任务中的相关计算需求。当计算从主机内，下方到了网络内后，这便释放了软件处理瓶颈，可以节约数据中心内宝贵的CPU处理资源。但主机内的计算任务并不能够全都被硬件卸载，因而我们需要选取可编程数据平面来完成这件事。前文提到过三类可编程网卡他们分别是：基于可编程ASIC的智能网卡，基于NPU的智能网卡，以及基于FPGA的智能网卡。首先本文分析基于ASIC的智能网卡是非“图灵完全”的可编程硬件，尽管其处理性能优异，但无法支持灵活配置，因而本文不考虑。其次基于NPU的智能网卡，NPU由众核处理器架构组成，可提升处理并行度，但这是基于批处理的计算模型。针对目前流式计算和有状态计算，因NPU每核处理性能低，从而导致NPU方案整体运行性能差。

本文从上述两类问题中各选取了一个应用场景，来说明使用基于FPGA的智能网卡具备强大的“每包处理”能力，以及强大的“复杂计算”能力。最重要的是FPGA可以支持网络内流式计算模型，本文在后面提出了一种在FPGA针对流式计算需求的DC抽象方法，可以将有前后状态依赖的“复杂计算”任务卸载到基于FPGA的可编程硬件。

%The Case For In-Network Computing On Demand 分类举例说明



\BiSection{系统架构}{aa} %1

\BiSubsection{软件向硬件卸载分析}{aa}

CPU通过循环取指令等操作，完成通用的可计算任务。计算的数据通常有前后依赖关系，CPU对于此类计算效率较低：由于中间计算状态在计算完成之后必须放回数据存储区，而下次重新拿回状态数据又需要再次搬运，数据在计算核心与存储池之间多次往返对计算最终结果是无意义操作。而即使使用众核处理器也无法优化此过程，虽然众核处理器可并行计算多个任务，但同一时刻每核心与其他核心处理内容并无关联。对于前后有依赖关系的处理，并行并不能加速其中一组数据的处理进程。在网络领域亦是如此，网络数据包到达密度很大，留给每个包的处理时间很有限，然而通常一个CPU无法在如此短时间内真正处理完一个包的触发计算。


1）软件处理延迟大。

目前高速网络处理器可以设计为核间流水线模式，每个核只处理固定的一步计算。当一个数据包触发的计算包括多个前后依赖计算时，人们使用多个核串行处理这组计算。即每个核心领取一个固定的快速处理任务，这样每个核都可以以最快的速度处理完当前步骤，然后交给后续核心继续处理。这样数据包的处理吞吐其实就可以达到某一个核心的最大速度，而处理延迟则是这些核心之间传递完整一次的时间。不难发现，数据在众核之间搬运会遇到访存时间过长、访存请求冲突等现象。虽然使用CPU可以获取很大的灵活性，但这种方式进一步增大了每个数据包处理的时延。如今在云端加速AI计算的场景下，高频次小包、数据快速到达的需求越来越高，这使得以CPU处理网络数据包造成很大的云计算性能瓶颈。后续我们通过本文所举的例子可以明显提现这一点。

2）软件处理时间精度低。

同时，由于数据在核心、存储池之间搬运会造成不定时的请求冲突、甚至计算等待，这将打乱顺序串行处理节拍。由于这条处理链中的处理速度，收到这条处理链最慢处的制约，因而处理链的性能表现总以最慢的瓶颈向外表现。
软件的处理收到操作系统的指挥，操作系统一般会划分时间片区来分配给每一个待处理的任务。虚拟化的操作系统内有很多任务进程，操作系统会划分很多时间片区。这也进一步降低了CPU的专用任务处理性能，还带来处理时间精度不足的现象。后续我们通过本文所举的例子可以明显提现这一点。

3）软件卸载。





\BiSubsection{软件算法的硬件抽象方法}{aa}

硬件编码思想
%阿里巴巴 大数据page70 流式计算的特点

\BiSection{流量工程---网络流量捕获与回放}{aa} %4

\BiSubsection{设计}{aa}

1Gbps 硬件设计思路

\BiSubsection{优化}{aa}

向100G出发


\BiSection{统计---网络测量实时压缩}{aa} %4

\BiSubsection{设计}{aa}

1）压缩算法

2）压缩算法硬件实现

3）效果与问题



\BiSubsection{优化}{aa}

1）符合DC抽象的算法优化方法

2）无偏估计证明




\BiSection{软硬一体化的系统实验平台}{aa} %2

\BiSubsection{软件}{aa}

\BiSubsection{硬件}{aa}

\BiSection{性能评估}{aa}

时间精度

吞吐

估算

cacti比较











在主机端，网络与计算的需求，

在第二章中提到NPU的可编程性最强且可以使用软件控制，由于他单核性能问题，尽管需要使程序员更换硬件思维，但业界普遍已经采取FPGA的方式来加速网络计算。

本章的主体是网络功能使用可编程硬件加速。

主机端网络开销最大的还属流量工程和网络内计算，因为每包处理，目前百G NIC 包速率已经达到150Mpps,对于每包操作的计算任务，CPU负载很大

1）分析软件到硬件卸载的可行性， 背景
	
	软件可扩展分析，



	

%流式处理《阿里巴巴大数据》page70


















