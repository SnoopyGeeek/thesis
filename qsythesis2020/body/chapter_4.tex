% !Mode:: "TeX:UTF-8" 

\BiChapter{研究可编程设备加速网络硬件交换层方法}{Research on Programmable Devices to Accelerate Hardware-based Core layer } \label{chap4}


\BiSection{本章引论}{Introduction to Chapter}\label{chap41}

本章在第\ref{chap42}小节分析了目前各类网络可编程数据平面的缺陷，并提出了一种新的数据平面可编程的交换机架构，包括编程模型（\ref{chap43}小节）以及硬件原理（\ref{chap44}小节）。接着利用前文\ref{chap332}小节提出的设计原理，在\ref{chap45}小节阐述了如何优化了FPGA内部的逻辑资源，证明了其配置方法属于NP难问题（附录\ref{fulua}），而后给出了一种高效的启发式算法（\ref{chap46}小节）。最后给出了系统的三个实际用例测试，证明了系统将数据平面的灵活性与性能同步提升（\ref{spe4}小节）。



\BiSection{问题背景}{Background}\label{chap42}

可编程网络硬件可以运行传统部署在服务器内的功能，从而将网络性能提高数个量级。由于软件定义网络概念的提出，网络灵活性增强，运营商更倾向于自主管理网络内设备的所有行为。具有灵活性的管理方式可以大大增加网络的运行效率，为特定的功能定制专用的网络场景。  现有的面向网络通信的可编程数据平面一般有x86平台，开放接口配置的ASIC转发芯片，以及上一章本文提到的FPGA平台。其在处理灵活性、转发性能等方面均有长足进步，但如前文\ref{chap232}节中介绍，在面对转发容量进一步提高，可配置能力进一步开放的网络应用时依然面临新的挑战：

1）软件具有高度灵活性，但处理性能低下

数据包交换对于CPU架构平台来说是一种很低效的机械劳动。在软件层面，数据包转发算法已经被优化得极为高效，但面对无穷无尽的任务量，依靠CPU指令集的处理架构存在访存效率低、无法批处理等问题。CPU无法发挥自己实现灵活程序跳转、分支的优势，因此目前高性能软件网络处理性能也只能达到10Gbps（单核心）。对于现代服务器面向200G接口，汇聚层交换转发动辄5、6Tbps的性能需求是远远无法满足的。

2）基于FPGA可编程硬件平台

FPGA是一种逻辑可编程芯片，既能够提供软件一般的可编程能力，也具有类似硬件的性能。目前，主要的云计算厂商阿里巴巴、亚马逊、微软都在数据通路内部署了FPGA加速引擎以同时满足性能和灵活性的需求，例如提升网络加解密性能，定制传输层协议等。然而由于FPGA电路编码转换采用“查找表+内部互联网络”的原理，为保证时间同步性，使得FPGA综合对外的处理主频只能达到200MHz，这样即使FPGA内流水线每个周期都能处理一个数据包，总吞吐性能也无法超过100Gbps\citeup{wang2017p4fpga,netfpga2014}。虽强于软件，但远差于网络的核心层性能需求。

3)协议无关概念的交换芯片

与协议无关交换架构（Protocol-Independent Switch Architecture,PISA）相对应的是P4（Programming Protocol-Independent Packet Processor）。P4是一种专用的编程语言，其目标是为任意包协议提供一种基于ASIC硬件（PISA）的现场可重配置能力。根据本文在第二章的介绍，PISA有能力实现自定义包头解析，自定义流表的组成结构，并且最重要的它拥有强大的处理能力（12.8Tbps，显然是唯一可以胜任核心转发设备的架构），并在市场上得到了良好的应用反馈\citeup{miao2017silkroad}。虽然这大大解决OpenFlow编程能力不足以及其设计本身所带来的可扩展性差的难题，但它不具备真正意义上其所追求的“图灵完备”可编程。例如：（1）数据通路内的处理动作只能被数据包触发，而无法响应其他行为。针对一些QoS场景系统更希望根据队列深度做出一些响应，比如拥塞控制场景下的控制算法NDP\citeup{handley2017re}；（2）PISA缺乏计算、控制属性的指令集，例如乘除法、灵活分支判断等；（3）PISA为无状态转发的可编程流水线设计，则对状态协议处理以及有状态计算造成了困难（防火墙\citeup{zerkane2016software}等）。

PISA转为通用型的无状态转发同时提高了性能与灵活性，本文将对PISA架构芯片的目标处理内容做进一步扩展。本文的目标是设计一种适用于网络交换层的交换机架构，这种架构可从计算平面高灵活性的工作任务分离和卸载到网络中，包含但不限于有状态转发、复杂计算以及自定义触发方式。



\BiSection{系统架构介绍}{System Architecture Introduction} \label{chap43}

P4是目前广泛使用的可定义数据平面内的数据包解析和查表过程的语言。P4将流水线外的功能需求通过P4\_extern模块来让用户自行定义。然而P4\_extern的编程范围只在PISA提供的执行器序列之内，例如校验算法，位移，插入数据等。目前的PISA芯片架构对于“PISA\_extern”的功能需求没有支撑能力。一种显而易见的解决方案是当需要一种新的“PISA\_extern”时，重新设计一种支撑新特性执行器的ASIC硬件。但这与PISA高灵活性的设计初衷相矛盾，从经济实用性角度分析也是不合理的。

本文提出一种硬件异构型的交换机架构，可以支持任意P4\_extern所定义的功能。本文将这种结构命名为“自适应交换”，首先本文需要解决如何设计硬件结构使得灵活性和性能可以充分展现，其次本文需要解决如何开发编程语言以映射到自适应交换的数据平面。最终为了展示自适应交换的高扩展性，本文将上小节内提出的PISA难以应用的场景均在自适应交换平台中实现。

\BiSubsection{架构设计}{Architecture Design}\label{chap431}

\begin{figure}[!ht]
	\centering 
	\vspace{-1.5mm} 
	\includegraphics[scale=1]{asarch.pdf}
	\caption{自适应交换结构框图} \label{fig:asarch}
\end{figure}

自适应交换的硬件架构框图。自适应交换包括两部分，1）固定功能的ASIC交换系统（Switching System, SS）；2）用户定义可编程逻辑块（Programmable Logic, PL）。SS基于标准的交换芯片（switching ASCI）处理功能，PL利用FPGA类的可编程硬件支持用户自定义逻辑。一个标准的交换芯片功能包含了数据包头的解析，基本的ACL功能和各个字段的匹配以及执行转发/丢包等动作。额外还会包括流量管理、队列调度等QoS功能。架构中SS与PL两部分可由双芯片拼接的方式实现，SS部分利用一个标准传统的交换芯片（既可以支持P4编程，也可以不支持P4编程）构成。PL端使用FPGA芯片或由FPGA构成的多用途片上系统（MPSoC/ACAP）芯片。在双芯片拼接方案中，两个独立的芯片系统须通过高速互联总线相连接，例如PCIe接口，以太网接口或者类似的收发器。相应地自适应交换系统也可以由单独一颗芯片构建而成，PL与SS部分通过片上高速总线相接，例如，AXI协议总线。

如图\ref{fig:asarch}左半部分所示，交换网络是SS的核心组成部分。交换网络支持网络入端接口与出端接口多对多的数据包互联传送，一般由交叉开关（cross-bar）构成。数据包经由物理网络接口进入SS系统，在交换网络两边，数据包会经过入端（Ingress）处理流水线，以及出端（Egress）处理流水线。在两条流水线的组成分别有包头解析（抽取必要的包头域数据），流表（对包头域数据进行匹配和执行相应的动作集），封包（重组/修改数据包结构），以及流量管理（包缓存/调度/限速等）。

数据包进入自适应系统时，首先经由SS端的处理，通常情况下大部分数据包可以完整地被SS处理，并返回外部网络。只有在SS内的功能无法满足需求的那部分数据包会再次送入PL端做协同处理。对于送往PL端处理的数据包，在SS端在片上存储中（或使用片外存储）也保留有一份完整是数据包备份，SS只将数据包的元祖数据信息送给PL。元祖数据中包括了PL完成处理所需要的定制化的包头信息或者其他描述信息。PL处理完成元祖数据后，会更新重写元祖信息中的包头以及描述字段，再返回给SS。最后SS将备份的原始数据包与更新的元祖数据重新整合成一个完整的数据包后再次对其执行转发操作，或简单丢包。

本文将在传统的交换芯片中增加一个包存储管理机制（Memory Management Unit, MMU），图\ref{fig:asarch}中左边虚线内部所示。当数据包元祖信息送往PL处理时，这个数据包在SS端的完整备份由MMU管理。MMU应包含三个主要功能：（1）动态申请/释放数据包的存储块位置及其大小；（2）驱动数据包向内存中读写时序；（3）读出备份数据包后与从PL返回的新元数据对接重组。

上述自适应交换流程与原理是建立在本文对以下两方面的分析结果和假设之上。首先，在PL中处理的信息通常只依赖于包头，或者数据包包首部分的字段。在设计中被交换到PL的元数据可以被灵活地定义，并且保证只占用SS与PL之间一定量的通信带宽成本。在某些极端的例子中，如果处理过程需要数据包所有的数据时，元祖数据也将会包含一个完整的数据包内容。第二，不是所有的数据包处理都需求调用PL端的功能。否则，如果有一种功能是普遍适用的，则一定会集成在现行的通用交换芯片内。或者这种功能需求可以直接拆分出来一部分由SS端完成处理。

自适应交换的高性能来自于对网络通用数据包包长的分析。本文考虑到，目前网络内平均包长度为600字节左右，假设元祖数据包含整个包头部分（64字节之内），则SS与PL之间的通信成本只占需求流量的10\%。即，利用单插槽的PCIe4.0接口（256Gbps）作为高速互联，自适应交换系统可以将目前最高速度（12.8Tbps）交换芯片中20\%的数据（>2Tbps）卸载到用户硬件可编程逻辑中进行处理。这远远超过了（$\times$10倍）单纯由FPGA组成的可编程数据平面的处理性能，也是本文研究的最主要的动机。

\BiSubsection{开发流程}{Develop Flow}\label{chap432}

对于基于FPGA的PL端，基本的开发设计流程包括如下几部分：

\begin{itemize}
	\item 定义数据包处理需求以及所需要的数据流模型。
	\item 编写处理规则对应的函数或流程代码。程序代码可包括高级语言如P4/P4\_extern、P4\_FPGA，或者底层的硬件描述语言如verilog HDL。此外包括数据平面高层次生成系统，如SDNet\citeup{sdnet}，也可以用于加速PL端的开发工作。
	\item PL端的机器码编译器。不同PL目标器件下的编程，往往有不同的编译步骤。对于基于FPGA的编译器，本文使用FPGA芯片厂商提供的编译环境。但本文最主要的贡献是解决如何将数据包流处理模型在异构形态下重新组织，并且完成高性能的异构形态下包处理逻辑无语义偏差的拆分和翻译。
\end{itemize}

\begin{figure}[!ht]
	\centering 
	\vspace{-1.5mm} 
	\includegraphics[scale=1]{asdev1.pdf}
	\caption{基于SDNet工具链的FPGA-PL端编译流程} \label{fig:asdev}
\end{figure}

本文利用Xilinx SDNet/P4-SDNet作为建立本文设计原型系统的基本开发工具，基于FPGA的PL编译流程如图\ref{fig:asdev}所示。SDNet和P4-SDNet已经是成熟商用包装解决方案，并包含了为FPGA数据平面设计的P4语言到verilog硬件模块的编译工具链。P4-SDNet内建两种专为P4$ _{16} $（提取包头数据，修改包头域值）提供的P4\_extern功能。这种extern功能给用户提供了足够灵活的直接修改原始数据包头域的能力，但目前此编译器并不支持其他extern对象的编译，且用户也不可定义其自己的extern对象。

使用其类似的思路，本文在开发流程中增加新的编译模块，使其将能够使用户扩展extern对象定义的范围：扩展前端编译器，使其支持其他的高层次描述（描述中增加标记符以提高后端编译器的性能），并将其转译为逻辑中间表示层（IR）；最后通过后端编译器再次映射到PL端。
本文提出的硬件转发体系结构与通行产品不同，下面将详细介绍本文开发流程与经典可编程数据平面（以P4为例）之间的差别。

\begin{figure}[!ht]
	\centering 
	\vspace{-1.5mm} 
	\includegraphics[scale=1]{ascomp.pdf}
	\caption{基于SDNet工具链的FPGA-PL端编译流程} \label{fig:ascomp}
\end{figure}

图\ref{fig:ascomp}中左边为经典的P4程序编译标准流程，对P4目标器件编程需要两个描述文件。首先是P4行为描述程序代码，包含了待处理数据包的包头域定义，包头域匹配关系图，流表设计，执行动作设计等。第二为P4架构图，此文件由设备厂商提供，编译器须根据不同底层芯片实现样式进行有针对性的编译，这样编译器可根据结构图对程序进行等效变换以及重组。前端编译器会将文本代码转换为逻辑关系表，也称作中间表示层（Intermediate Representation, IR），后端编译器一般有厂商提供，可将符合规范的IR表示直接转为机器二级制配置文件。远端控制器将配置文件下载入可编程交换机设备即完成了数据平面重配置过程。此时，控制器也需要收集P4定义信息，以方便后续添加流表项等网络功能设置。

根据之前本文提到的自适应交换系统的额外特点，如图\ref{fig:ascomp}右边所示，本文对基础编译框架进行补充。首先P4结构图需要修改为自适应交换机的底层实现，例如需要体现包头域协同处理机制。其次本文支持高层次语言的PISA\_extern对象编程，因此在额外扩展的前端编译器中，需要将高层次编译器适配入本流程。新增部分由红色区域块所示。最终后端编译器配合P4结构图生成FPGA中的RTL代码，交由FPGA厂商工具链生成二进制流文件（bitfile），通过板上运行时代理下载入FPGA即完成了PL端配置。SS端配置同经典过程所述。

\BiSubsection{高层次语言映射样例}{Examples of Mapping High Level Languages}\label{chap433}



{\fontsize{10pt}{0.5\baselineskip}\selectfont
	\begin{lstlisting}[caption={数据平面接口：PL端高层语言描述},label={ascode1}]
	extern extern_example{           				//声明模块名称：extern_example
		ext_type(in bit<wdth> input_Port);			//声明接口位宽以及数据流方向
		int a = VHDL_method_example();				//模块使用其他语言快verilog
		void CPP_HLS_method_example(a);				//模块使用类C的函数描述语言
	}
	control control_example{						//实例化样例模块
		extern_example(0x0) my_extern_example;
		action my_extern_call(){					//调用PL端功能
			my_extern_example.C_HLS_method_example();
		}
	}
	\end{lstlisting}
}

PL以FPGA芯片为例，自适应交换平台可以将SS交换芯片无法完成的用户定义功能映射到FPGA中。用户的额外功能对象须遵循PL端输入输出接口规范。P4语言风格的数据平面接口规范如代码 \ref{ascode1} 所示。代码定义了一个名为“extern\_example”的PL模块，模块可由C语言或HDL风格实现。对外接口位宽以及方向需要标记。在后期可实例化一个或多个样例，每个样例可自由调用PL端模块中的任意描述功能。

{\fontsize{10pt}{0.5\baselineskip}\selectfont
	\begin{lstlisting}[caption={extern实例的C++类声明},label={ascode2}]
	#include <hls/hls_sim/extern.h>					//头文件extern.h 定义了外围数据传输
	using namespace std;
	template <typename .. Args>
	using ActionPrimitive =
		hls :: ActionPrimitive <Args ...>;
	using hls :: Data;
	class extern_example: public ExternType{
		public:
			void init() override{}
			void C_HLS_method_example(){...}
	};
	HLS_REGISTER_EXTERN(extern_example);
	int import_extern_example(){return 0;}
	\end{lstlisting}
}

如代码 \ref{ascode2} 展示了C++风格extern函数。头文件“extern.h”中定义了核心函数之前的流水线结构（需要用户注意并修改），无需提现在核心描述代码中。FPGA高层次编译工具链（SDx，Bluespec System Verilog）可读取此类文件，并综合成带有用户功能的IP核，小的IP核可便于集成进更大的FPGA工程。

{\fontsize{10pt}{0.5\baselineskip}\selectfont
	\begin{lstlisting}[caption={extern实例的VHDL模块声明},label={ascode3}]
	`timescale 1ns/1ps
	module VHDL_method_example#(
		parameter DATA_WIDTH = D_WIDTH,
		parameter CTRL_WIDTH = C_WIDTH,
		parameter EXTERN_REG_WIDTH = R_WIDTH,
		...
		)(//---data interface
			input		[DATA_WIDTH - 1:0]	in_data,
		  //---register interface
		  	input [EXTERN_REG_WIDTH - 1:0]  in_reg,
		  //---misc
		  	input							clk,
		  	input							reset);
		  	
		//local parameter
		//wires/regs
		//modules
		//method example logic
	endmodule //VHDL_method_example
	\end{lstlisting}
}

代码 \ref{ascode3}展示了VHDL语言实现PL实例模块，在VHDL中须描述数据总线位宽，寄存器通路接口等。对于其他硬件系统，用户只需要指定通信用同步数据总线的时钟周期（时间约束）。
实际上，本文所描述的所有编译工具链仍处于设计阶段，并需要大量代码工作完成自动化处理。本文主要目的在于规划框架以及作为商用工具链的补充设计参考。硬件开发时间周期过长，为快速进行数据平面验证，本文生成的所有bit文件均为手动整合代码。

\BiSection{硬件设计}{Hardware Design}\label{chap44}

\BiSubsection{协议设计}{Protocol Design}\label{chap441}

\begin{figure}[!ht]
	\centering 
	\vspace{-1.5mm} 
	\includegraphics[scale=1]{asframe.pdf}
	\caption{SS与PL内部通信帧格式} \label{fig:asframe}
\end{figure}

上节提到自适应交换架构中SS端与PL端的通信须包含元祖数据以及包头，本节论文详细讨论通信帧格式。本文设计了一种通信帧（可满足大部分需求，用户也可以自行定义），格式如图\ref{fig:asframe}所示。首先帧头指定了帧的编号PID，帧由SS中的数据包生成，完整的数据包与帧使用同一个PID编号，方便PL协处理器处理之后重新组合。元祖信息（metadata）包括了数据包转发过程中的信息，例如输出端口，组表编号，丢包等。用户元组信息为帧有效字长，由于用户可以定义帧长度，这将给用户极大的灵活性空间。此外用户还可定义包头长度以及携带的数据包负载信息长度等。SS与PL之间传递的大部分操作数都是商用ASIC中已经实现的指令，因此SS端设计可以极大程度复用已有芯片设计。须增加的功能只有帧长度信息等域，这样可使对ASIC改动降到最低。

\BiSubsection{SS端固定功能}{Fixed Functions in SS}\label{chap442}

SS端对于目前商用普通交换芯片中需增加的功能为第一小节中提到的图 \ref{fig:asarch}内左边虚线内的MMU模块。自适应交换的核心思想是利用基于ASIC的交换系统（SS）提供包交换的高性能，而用可编程硬件协处理器（PL）提供包处理的灵活性。因此在SS端本文将包处理逻辑做的最简化，只保留一个用户可配置逻辑（即帧长度的定义）。MMU将用户定义的不超过帧长度的包头数据（指某一区间段）复制到通信帧中即可。MMU的其余工作还包括了动态申请和释放数据包缓存空间，以及重组数据包。接下来本小结主要介绍MMU的分析与设计。

假设SS与PL之间单向通信带宽有256G(PCIe4.0)，以帧内包含完整64字包头节为例，则包速率达到380Mpps。目前交换芯片的时钟驱动频率大约在1GHz到1.5GHz之间。对于MMU模块，若留给每个数据包的处理时间3周期则不会超过时钟频率（需要约1.2GHz主频）。在之前的MMU设计中，内存被分割为每页256字节。调度器以页为单位进行分配，维护逻辑映射与物理映射之间的关系由于页容量比较小而变得比较复杂。因此MMU瓶颈处理一个分配任务需要耗时25周期\citeup{wang2017p4fpga}，尽管这种设计思路在资源利用率上比较占优（平均资源浪费仅为1/2page），但它的时间复杂度太高，导致分配性能受影响。在ASIC的存储设计中，其经济性要好于由FPGA组成的片上存储资源。因而本文在设计存储单位时将每个地址对应的容量配额扩大为1.6KB（可完整放下一个最大包）。在分配以及查找长包时，可节省逻辑地址与物理地址之间多次正向/反向映射过程，因此大大提升分配速度。

\begin{figure}[!ht]
	\centering 
	\vspace{-1.5mm} 
	\includegraphics[scale=1]{asmmu.pdf}
	\caption{高pps性能MMU的实现方案} \label{fig:asmmu}
\end{figure}

如图\ref{fig:asmmu}所示，本文使用了一组“标志位”寄存器来指示其对应的page空间是否被占用。当数据包请求一个新的可用地址时，优先译码器会将标志位最左边为“1”的位置作为地址（address）返回给调度器，且便立即将此位置的标志“1”更新为“0”。之后调度器会把数据包向此位置写入，并且令此位置的地址作为PID值填入PL通信帧的首部。当MMU收到一个含有PID值的读请求时，MMU会读取以PID值为位置的存储空间并将其组合为数据包重新发出到入队流水线中，完成后立即将此位置的标志位为“1”。此时数据包的metadata中一般包含了由PL端处理完成给出的执行需求，SS端无需对此包做额外的分析工作。为了防止地址冲突以及标记位数据不同步带来的读写错误，优先译码器必须在完成上一个“读/写”请求之前，更新空闲地址，以备调度器无缝衔接下一个写请求，读请求则无需使用优先译码器结果，无操作风险。上文分析过，对每个读写操作，MMU都预留了3个时钟周期的处理时间。在系统运行后，由于数据包连续到达以及连续读写，因此在一段时间内，平均读次数是等于写次数的。MMU将读写交替进行，可为写请求争取6个时钟周期的译码时间，一般来讲对于有4k个位置的优先译码器，6个时钟周期足够通过硬件并行地两两比较找出最优位置（每周期比较两次的情况下$2^{6\times2}=4096$，但一般可以比较次数更多）。

接下来分析MMU应维护的内存容量大小。在元祖信息帧由PL返回之前，数据包都需要维持在MMU的存储空间内。假设信息帧处理时间为$T=n$(（微秒），上文提到总体包处理频率不超过$v=$380Mpps，因而平均在等待的数据包个数为$T \times v \leq =380n$。目前板间硬件处理时延应保证在3微秒以下，如果使用片内AXI总线互联，则完整处理时间应保证在1微秒\citeup{xilinxpcie}以内\footnote{在数据包的处理过程中查找TCAM表是最费时间的，一般需要20个时钟周期，但正常处理流程内TCAM表项不会超过2个。普通精确匹配查找表只需要不超过3个时钟周期即可。我们假设比较悲观的情形，PL内的处理过程包含5级TCAM查表以及10级精确匹配查表，则总共须耗费650时钟周期。若以200MHz主频运行FPGA，则总耗时约0.7微秒（一般只需要0.1微秒）。因此本文认为，PL的处理时延主要由信息帧传输过程贡献。PCIe收发器结构复杂，突发的批次传输时延较大（100时钟周期\citeup{xilinxpcie}），但高性能片上AXI总线的传输时延也很低（不超过10时钟周期）。}。为保险起见，本文将处理时间余量增大十倍假设最大不超过10微秒，则根据上式，可约估SS端等待数据包个数为4k个。上文提到每个数据包存储位置占用1.6KB，则板上共计须设置最多为6.4MB存储空间。以目前的半导体工业水平可满足此需求。 



\BiSubsection{PL设计}{PL Design}\label{chap443}

基于FPGA的PL设计最大的挑战来自于大批量信息帧的处理速度（380Mpps）。对于PL内部逻辑，由于帧内包含了基本的metadata信息以及包头域段落，对PL的性能设计可将其按照网络处理最小包（64字节）的方式分析。按照FPGA主频200MHz计算，即使每个时钟周期都能由流水线处理完成一个数据包，FPGA的性能也只能达到200Mpps。对于某些带状态阻塞性处理机制（如有状态查找表），平均需要2-3个时钟周期才可处理完成一个数据包，因此FPGA的处理性能会下降到60Mpps。远远达不到系统设计的需求。

一种思路是将FPGA内一条流水线完整复制多份，以均分总体任务量。例如可以将流水线同时复制$K$份，平均总处理性能是每条子流水线的$K$倍。然而这也意味着总资源提供量需要耗费之前每条流水线的$K$ 倍。但由于数据包流水线内有查表等大资源消耗量组件（1k深度的TCAM表项可占据整个FPGA逻辑资源的1/3），有限的FPGA逻辑面积不可能满足多条并行流水线的资源占用需求。

为解决上述挑战，本文提出一种微结构并行数据包处理流水线设计（micro-level parallel processing）。微结构流水线的核心思想是将原有的大表项拆分为多个小表项的等价组合，即通过不完整的复制大资源模块，而满足原始用户语义下的数据包处理需求。总结如下，本文将通过以下两个关键技术解决高效PL处理设计，1）提出一种资源高效的并行数据包处理架构，2）通过流表编译算法将流表项映射到多路子流表中，以达到流量均衡并高效利用硬件资源。

1）资源与性能

查找表是数据包处理过程中最核心的部件，也是资源消耗最大的组件\citeup{zheng2006tcam}。为提升效率本文需要依靠并行子流水线的思路扩展数据包处理性能。同时，为满足资源消耗限制的约束，本文需要分析如何拆分流表。另外，多级流表是目前交换系统中流水线的重要设计特征。多级流表可以降低单张表的资源消耗，但每条子流水线中只包含部分信息，无法保证同一条流水线内数据包的完整处理诉求。 




2）并行流水

流水线被分为$K$个并行子流水线后，数据包流也应被尽量平均地分类为$K$份。然后将不同的组份以流量均衡的形式放置在不同的子流水线中。本文将数据包按照包头信息内的ID比特位来分成$j (j > K)$个组份。例如，将包头信息哈希为$n$ bits数值，以此数值来代表不同的流组份（根据哈希函数定义，同一条流的所有数据包均可分到同一个组份），则共可分成$j=2^{n}$份，显然每一份的流量大小是不一定相等的。如图\ref{fig:pljidk}所示，本文将j个ID组份以流量均衡的形式安置在K条不同的流水线中。

\begin{figure}[!ht]
	\centering 
	\vspace{-1.5mm} 
	\includegraphics[scale=1]{pljidk.pdf}
	\caption{$ j $组ID组份向$ K $个集合中分配} \label{fig:pljidk}
\end{figure}

为确保每个$K$集合中流量均衡，可调整$j_i$的排列，若假设$ j_i $为第i个组份的流量大小，使得每个集合之间的总流量大小相近。则集合总流量$ K $可表示为：

\begin{equation} \label{pl1}
K_0=\sum_{i_0}^{k_0}j_{i_0}
\end{equation}

3）流表拆分

本文首先讨论在PL端不同类型的流表拆分原则，然后讨论如何使拆分表组合为多级流表之后的语义保持不变。查找表从功能以及实现方式可以分为精确匹配和掩码匹配。由绪论部分可知，精确匹配目前有两种实现方式：（1）内容地址映射表（CAM），（2）基于RAM的查找表；掩码匹配可由TCAM实现。

前文提到，数据包在进入子流水线之前须经过哈希函数（hash()）分类，并确定最终去往的目标子流水线。将流水线分为$K$份，则流表也需要拆分为$K$份。哈希函数将全体数据包流分为了$j$份，因此利用哈希函数也可以将流表内容分为$j$份，并按照集合$K$中的分布，将流组份$j$对应的所有流表项也分配到$K$个流水线中。下面依次介绍每种表的拆分方法：

\begin{itemize}
	\item CAM表。此过程对于精确匹配的CAM表的分类比较直观，只需要按照哈希结果，将CAM表中每一条流表项分到对应的K集合内，亦可保证语义正确。
	
	\item LPM（Longest Prefix Matching，最长前缀匹配）表。与CAM不同，PLM表内含有掩码位（*）。不同的数据流被哈希函数分到不同的组份，由于不同的组份有可能被分到不同的子流水线，有可能导致被同一条LPM表项同时匹配的两条流被分到不同的子流水线。若按照无复制方式去拆分流表项，则必然造成一部分流量无法匹配成功。如图\ref{fig:plmask}所示，在做流量分类时，选取包头域中四个bit位进行哈希取值。若这些bit位中有位置对应到了带掩码表项（1*10），而此时，两条流“1010”与“1110”恰好又被哈希函数分到了两个子流水线中的组份（$j_1$与$j_2$），则此掩码表项须分别复制到两条子流水线中。
	\begin{figure}[!ht]
		\centering 
	\vspace{-1.5mm} 
		\includegraphics[scale=1]{plmask.pdf}
		\caption{带掩码表项在拆分时，可能需要被复制多份} \label{fig:plmask}
	\end{figure}
	\\因此带有掩码的表项在做并行子流水线拆分时会占用比拆分前更多的表项空间。例如在IP路由查找场景下，通过分析掩码的位置以及流组成发现\citeup{zheng2006tcam}，适当调整包头域中抽取的bit位作为哈希函数的输入值，可保证多占用的表项空间维持在比较低的水平（6\%左右）。因为如果不选择带有掩码位置的bit为作为输入，则可化简为精确匹配的场景，因此也不会遇到无法匹配的问题。
	\item TCAM表。与LPM相似，根据哈希函数抽取bit位置是否带有掩码（*）位，TCAM表项也可能会占用比原始表项空间更多的流表项数目。
	\item RAM（直接查找）表。查表时以带搜索KEY的值为地址，直接读出RAM地址位对应的存储数据作为搜索结果。若搜索的数据位宽为$ KEY $位，则RAM表的深度应为$2^{KEY}$。由于RAM表的深度只与KEY的位宽相关，RAM表的存储依赖于连续地址空间，因此RAM表拆为$ K $个也无法使RAM表的容量缩小为原来的$1/K$。在第二章本文介绍过，由于使用RAM查表的域宽度一般不大，只有16位（查找端口号等），占用空间很小（不超过总资源的0.5\%），可以考虑完整复制。其次，若RAM真实利用量比较小，可以用CAM代替RAM表，从而更节约逻辑资源。
\end{itemize}

4）多级流表与流量均衡分类器

在数据包处理的过程中，假设匹配N个域，每个域共M个流表项，若只使用一张流表，则总共需要消耗$ M^N $深度的流表项。一般实际应用时，会为每个域单独设立一个查找表，多个域使用多级查找表以串行流水线方式完成匹配过程。这样只需使用$M\times N$深度的表项，可大大节约资源消耗量。

然而，通过第一条子流水线的数据包在匹配下一个域时，下一张流表的流表项不一定与此数据包处于同一条子流水线。如图\ref{fig:pllb0}所示，数据包p0根据流量均衡哈希函数被分配到第1条子流水线，经流表0匹配结束后，此数据包还需流表1继续匹配，但此时发现，处理p0数据包对应的流表1不一定依然存储在流水线1中，也许被拆分到了流水线K-1内。则数据包p0无法得到后续服务。

\begin{figure}[!ht]
	\centering 
	\vspace{-1.5mm} 
	\includegraphics[scale=1]{pllb0.pdf}
	\caption{流表分割引发的数据包处理信息丢失问题} \label{fig:pllb0}
\end{figure}

为解决此问题，本文提出一种新的“并行查找块结构”，对流量均衡哈希函数以及多级流表之间数据传递进行的重新设计。如图\ref{fig:plppl}所示，每一个查找块代替传统查找流水线内的一个流表（匹配0或匹配1）。查找块使用串行方式组成流水线。每一个查找块内都包含了流量均衡分类器。数据包首先进入分类器，分类器提取出包头域中特征位，将此数据作为哈希函数的输入值进行哈希处理得到一个8位ID数字。本文将此ID号码代表前文提到的流分类组份$ j $的值。接下来的RAM表中保存了对每一个ID编号应该由哪一个子表查找的信息。数据包得到子表编号之后，通过一个交换网络转发到对应的子表流水线中，并完成相应域的查找-执行动作。分类器中子表的映射关系直接关系到后续子流水线的查找任务负载程度，以及子表中存储的流表项数目大小。

\begin{figure}[!ht]
	\centering 
	\vspace{-1.5mm} 
	\includegraphics[scale=0.95]{plppl.pdf}
	\caption{PL端带有流量均衡的并行查找块结构} \label{fig:plppl}
\end{figure}

本结构解决了本小结前述的挑战，增加了并行处理性能，同时能够减少复制流表所耗费的资源空间。但也引入了一系列其他结构，如分类器，交叉开关以及多路执行机构。接下来本文分析这些子模块的设计复杂度以及逻辑资源开销。


\begin{itemize}
	\item 分类器。分类器由特征提取器，哈希函数以及一个查找子表组成。特征提取器和哈希函数结构简单，特征提取器在电路中使用一个固定位选路器即可实现。哈希函数使用一些异或门电路以及少量加法逻辑可实现，资源占用可以忽略不计。查找子表是本模块消耗资源最大的，但对于一般流量，只需要将其分为256或1024（$j$）类即可。其最多占用存储容量1KB，现代通用型FPGA的存储资源多大上百兆，因此也可以忽略不计。
	\item 执行器。执行器内无须占用大面积存储资源，一些处理和控制功能属简单逻辑，因此执行器所消耗资源也微乎其微。
	\item 交叉开关。交叉开关可为$ K $个输入通道与$ K $个输出通道同时互相连接。是一种高性能的信息交换机制。交叉开关为每个输入与输出接口都放置了一个连接开关机构，因此一个$ K $口输入输出交叉开关需要$K^2$个逻辑资源复杂度。本文在设计并行查找块时，其并行数量$ K $在4--8个之间，因此交叉开关的资源耗费量也在可控范围内。
\end{itemize}

本文将在本章的\ref{spe4}小结中给出以上几类模块的资源消耗，从数据可以看出，本文引入的模块与流表容量相比是可以忽略不计，因此设计是有效的。
本文对一般结构的数据包处理流水线做了两方面的优化。在横向上，每条子流水线包含了多级查找表设计；纵向上，每条流水线之间的同一级流表中流表项以分散的形式存储在子流表内。本文并不改变传统的查找表与执行机构的设计，只改变了其组织和使用方式，因此查找表依然可以使用目前工具链由高层次语言编译。因此在PL的编译中，本文的前端编译器只需要将流表定义转译为拆分后的结构。其他的辅助模块结构是固定的无需再修改，这也为本文的前端编译器设计工作减轻了负担。通过以上两种优化方法，本文可同时享受到并行包处理流水线带来的高吞吐量，也无需大量复制流表项，且能保证流表资源不成倍增长。


%
%有状态转发放到实验中去讲

\BiSection{流表、流量分配问题}{Allocation Problem of Table Entries and Flow}\label{chap45}

对本文提出的自适应交换系统的编程涉及到两个部分，一部分是分类器子表的配置，另一部分是流表项的配置。由前文分析可知，分类器子表的配置影响到后续子流水线中流量分配，以及子表中流表项数目的占用。而且如果更改子表配置，则后续对应的子流水线流表项也许同步进行删除和添加。分类器子表的配置是根据不同流组份流量大小来确定的。根据真实流量的分析\citeup{zheng2006tcam}，平均情况下一个流组份的流量大小基本保持不变。因此在得到最佳配置之前，可以先通过测量的方式来对分类配置进行实时调整。接下来本文分析分类表以及流表配置过程。

在哈希函数将数据包分类到j个组份之后，每个组份的平均流量大小是随机的，且不均匀。分配过程的主要任务是将各个组份分配到K条子流水线之后使子流水线中的流量尽量保持均衡。此外，每个组份所包含的流个数也不尽相同，但每一条子流表的深度是相同的（假设为原来匹配流表深度的$ 1/K $），分配过程还应注意到不同类别的流表拆分会导致流表多份复制，且应该使得每个子流表中所分配到的流表项数目均衡，并最大限度地减少流表资源的额外消耗。

未解决上述的多约束问题，本文首先将问题数学抽象：对于图\ref{fig:plppl}中所示的一个并行查找结构。假设$N_0$是这一级匹配拆分前所对应的原始流表项数目；$C_0$是原始流表容量深度；$R_d$是流表的资源消耗率；$N$是并行流表中流表中已消耗的资源总量；$K$是流水线并行度；$C_k(k\in [1,K])$是每个子表的设计容量；分组$ ID $号码由HASH()函数求得，且$ j=HASH(P~bits) $，输入数据是一个$ P $位宽度的数值，$ j $表示了$ ID $值的空间范围。

因此可以得到对于流表资源的总约束条件：

\begin{equation} \label{pl2}
N \leq \sum_{k=1}^{K}C_k \leq C_0 \times R_d
\end{equation}

假设 $D\_flow[i]$是网络中数据包流$ i $的流量大小，则有：

\begin{equation} \label{pl3}
\sum_{i=1}^{N}D\_flow[i]=1,i \in [i,N]
\end{equation}

$D\_id[j]$是属于同一个ID分组内的总流量大小：

\begin{equation} \label{pl4}
D\_id[j]=\sum_{flow[i]\in j}^{}D\_flow[i]
\end{equation}

$T$为同级内各个流表之间流表项利用均衡度差异（单位：条）。由于对FPGA内总资源的把控需求，可为流表多倾斜分配的资源有一个上限，可用$T$来表达。令$S$是组份j所组成的集合，$S={1,2,\cdots,2^P}$。$Q_k$定义为每个哈希后ID组份的最终分配情况，$Q_k \in S$，$\cup Q_k=S$， $|Q_k|$表示一个子流表中组份的总个数。令$D[k]$为第k个子流水线所承载的总数据包吞吐，则有：

\begin{equation} \label{pl5}
D[k]=\sum_{j\in Q_k}D\_id[j]
\end{equation}

如前文（流表拆分）中分析，一个带有掩码的流表项可被拆分到不同的组份$ j $中，但其中不同的组份还有可能被重新分配到同一个子流表内。因此可以将已经拆分开的掩码表项重新合成一条。定义总共被合并掉的表项个数为$V_k$。本文称待解决优化问题为“基于流量均衡的流表组合问题”（The Load-Balance-Based Table Construction Problem，LBBTC）。

\begin{definition}[LBBTC]
	$K$为系统并行流水线数目，当$k\in [1,K]$，若服从：
	%
	\begin{enumerate}
		\item 基本约束，$Q_{k}\subseteq S, k\in \left[ 1,k\right],\cup Q_{k}=S;\sum ^{k}_{i=1}C_{k}\left[ i\right] \leq C_{0}\cdot R_{d} $；
		\item 流表空间占用限制，$\left| \left| \dfrac {Q_{i}}{C_{ki}}\right| -\left| \dfrac {Q_{j}}{C_{kj}}\right| \right|\cdot 100\%\leq T\left( i, j, ki, kj\in \left[ 1,k\right] \right) $；
		\item 布尔函数，$ BOOL\left(i,j\right)=
		\begin{cases}
		1& \text{ $j \in Q_i$ }\\
		0& \text{ $j \notin Q_i$}
		\end{cases}$；
		\item $G\left[ j\right] =\sum ^{k}_{i=1}BOOL\left( i,j\right) =1\left( j\in S\right)  $。
		\item $\sum _{j\in S}G\left[ i\right] =2^{P}\left( j\in S\right)  $；
		\item $D\left[ k\right] =\sum _{j\in Q_{k}}D_{-}id\left[ i\right] \left( k=1,\ldots ,K\right)  $；
		\item 优化目标1，$ F_{1}\left[ k\right] =MAX\left( D\left[ k\right] \right) -MlN\left( D\left[ k\right] \right) \leq q_1$；
		\item 优化目标2，$ F_{2}\left[ k\right] =C_{0}-\sum ^{K}_{k=1}V_{k}\leq q_{2}$；
		\item 帕累托最小优化，$ F\left[ k\right] =\left( F_{1}\left[ k\right] ,F_{2}\left[ k\right] \right) \left( k\in \left[ 1,K\right] \right)  $
	\end{enumerate}
	%
	对于帕累托最小优化的求解定义为LBBTC问题。 \qquad \qquad \qquad \qquad \qquad \qquad \qquad \ \ \  $\Diamond$
\end{definition}


本文在附录\ref{fulua}中证明上述问题为NP难解问题，而后给出一种基于模拟退火的启发式算法。首先引入一个经典的NP完全问题“平均分割问题”，随后本文通过给出此问题的退化过程，证明“基于流量均衡的流表组合问题”是一个NP难问题。


\BiSection{算法设计}{Algorithm Design}\label{chap46}

本小结将给出一种启发式算法，使前文提到的NP难问题在可接受时间内得到一组有效解。简单解思路为暴力搜索所有$Q_k$的可能组合，来寻找一组均衡的流量分布结果。对于$N$个ID组份的计算搜索次数可以被估计为：

\begin{equation}
	T(N)=K^N
\end{equation}

其中$K$为并行子表的个数，当$N=16$时，计算搜索数约为$10^{10}$数量级，以目前服务器CPU计算能力约须消耗10秒钟才可找到精确解。然而在真实场景下，一般须设置$N\geq 64$（为使各个流水线中流量更均衡），搜索空间会陡增至$10^{38}$数量级，甚至可能将$N$设置为表达256个ID组份之多。

\begin{algorithm}[!h]
	\caption{LBBTC算法初始化  \label{lbbtcinit}}
	\IncMargin{2em}
	\DontPrintSemicolon
	\KwIn{流量统计结果$D\_id[i]$}
	\KwOut{初始化解空间$Q_k$}
	
	$Q_k = \phi; D[k]=0;i,j,k=0;i,j,k \in \mathbb{Z}$; $//$初始化
	
	对 $D_{-}id[i]$ 按降序排列，记为$\left\{ Sid\left[ 1\right] ,Sid\left[ 2\right] ,\ldots ,Sid\left[ 2^{P}\right] \right\} $\;
	
	\For{i in [1,$2^P$]}
	{
		\For{j in [Sid[1],\ldots, Sid[$2^P$]]}
		{
			对 $D[k]$ 按升序排序，获取各小流量$ID$组份，记为 $\left\{ Sd\left[ 1\right] ,\ldots ,Sd\left[ k\right] \right\} $\;
			
			\For{k in [1,K]}
			{
				\If{$\left| Q_{Sd\left[ k\right] }\right| =Min\left| Q_{i}\right| ,i\in \left[ 1,K\right] $}
				{
					$D\left[ Sd\left[ k\right] \right] +=D\left[ Sid\left[ i\right] \right] $\;
					
					$Q_{Sd\left[ k\right]} \cup =\left\{ Sid\left[ i\right] \right\} $\;
					
					\textbf{break}\;
				}
			}
		}
	}

$//$获得了流表容量占用均衡的一个排列: $\left\{ Q_{k},D\left[ k\right] ,k\in \left[ 1,k\right] \right\} $\;


重新回到按照$D\_id[i]$原始的降序排列，以下按照流量公平优先的方式分配：

\For{i in $\left\{ Sid\left[ 1\right] ,\ldots ,Sid\left[ 2^{P}\right] \right\} $}
{
	\textbf{find} $D\left[ k_{i}\right] =Min\left( D\left[ k\right] \right) ;k,k_{i}\in \left[ 1,K\right] $\;
	$Q_{k_i}\cup =\left\{ Sid\left[ i\right] \right\} $\;
	$ D\left[ k_{i}\right] +=D\left[ Sid\left[ i\right] \right] $\;
}

$//$获得了流量均衡的一个排列: $\left\{ Q^{'}_{k},D'\left[ k\right] ,k\in \left[ 1,K\right] \right\} $\;



\end{algorithm}

本文提出一种基于模拟退火方法的启发式算法。在PL的多级流表中，每一级流表组合机制的结果空间为初始集合S的子集$Q_k$。模拟退火算法的状态转移探索方式为下列情形二之一：（1）从集合$Q_k$中随机地移动一个元素到任意其他的集合$Q_{k'}$；（2）交换任意一组（2个）集合$Q_k$中的两个元素。首先，在每一个启发式搜索状态内，须保证$Q_k$内元素的有序性。如果流量都很均衡则哈希后的ID组份流量也会均衡，但考虑到真实网络流量的“80/20”分布定律，哈希后流量分布也呈现巨大差异性。差异性提现在两方面：第一是流量的不均衡性，第二是流数目分布的不均衡性。因此本文将解空间（$Q_k$）内的元素分为两个属性，一部分是流量巨大的ID组份，另一部分是小流所代表的ID组份。在第二种转移探索方式中，各集合$Q_k$内同属性元素做互相交换。例如，做集合$Q_1$和$Q_2$元素交换时，只选择每个集合里符合大流属性的ID组份间进行交换。大流组份不会与小流组份进行交换。因此退火算法可以避免大量的无效探索次数。

本文定义启发式算法的能量方程（评价函数）如下：

%\begin{equation}
%
%\end{equation}

\begin{align}\label{a4}
&E_1(n)=\dfrac{Max|D[k]-avg(D[k])|}{avg(D[k])\times 100\%}  \\
&E_2(n)=\dfrac{\sum{K}(C_k-C_0)}{C_0 \times 100\%}
\end{align}

模拟退火的核心优化思想是，接受一个新探索状态的条件是满足新的能量低于当前能量。在本文的启发式算法中，进入新阶段的移动条件是：

\begin{equation}
\left\{
\begin{array}{lr}
\Delta E_1= E_1(n+1)-E_1(n)<0, &  \\
and,& \\
\Delta E_2=E_2(n+1)-E_2(n)<0 &  
\end{array}
\right.
\end{equation}

另外，当满足移动移动条件时，算法最终以概率$P=e^{-|\dfrac{\Delta E_1 + \Delta E_2}{t} |}$接受移动推荐。温度参数$t$初始值为100，并且在之后迭代中以指数函数形式（$t(n+1)=t(n)\times 0.99$）下降到1。如算法\ref{lbbtcms}所示，最终停止搜索的跳出条件为：

\begin{equation}
\left\{
\begin{array}{lr}
E_1<q_1\  \&\& \ E_2<q_2, &  \\
or,& \\
t=1 &  
\end{array}
\right.
\end{equation}

如算法\ref{lbbtcinit}描述了退火算法初始化的过程，算法\ref{lbbtcms}描述了模拟退火算法根据初始化值进行元素交换、退火、最后退出的过程。

\begin{algorithm}[!h]
	\caption{LBBTC启发式算法  \label{lbbtcms}}
	\IncMargin{2em}
	\DontPrintSemicolon
	\KwIn{初始化后的两组解空间$Q_k$、$ Q^{'}_{k} $及其对应的流量统计和$D[k]$、$ D'\left[ k\right] $}
	\KwOut{经过元素交换收敛后的答案解空间$Q_k$}
	
\textbf{define:}$\Delta(D,K) =\sum ^{K}_{k=1}\left| D\left[ k\right] -\dfrac {100\% }{K}\right| ^{2}$\;

\If{$Max\left( \left| Q_{k}'\right| \right) \leq C_{0}\cdot R_{d}/K$}
{
	\Return $\left\{ Q^{'}_{k},D'\left[ k\right] ,k\in \left[ 1,K\right] \right\} $\;
}
\ElseIf{$Max\left( \left| Q_{k}'\right| \right) > C_{0}\cdot R_{d}/K$}
{
	\textbf{set} $ C_{k_{1}}=2 \cdot C_{k_{1}}$\;
	\While{$C_{sum}>C_0 \cdot R_d$}
	{
		\textbf{find:} another $C_{k'}$ and 
		
		\textbf{set:} $C_{k'}=\dfrac{C_{k'}}{2},k'\in [1,K]$\; 
	}
	
	\While{$|Q^{'}_{k}|>C_k$}
	{
		\textbf{pop:} a $ID$ group from $Q'_{k}$ to  $Q'_{k_1}$\;
	}
}

计算： $\Delta(D,K)\  and\  \Delta'(D',K) $\;
\If{$\Delta>\Delta'$}
{
	\textbf{return} $\left\{ Q^{'}_{k},D'\left[ k\right] ,k\in \left[ 1,K\right] \right\} $\;
}
\Else{\While{$Times--$}
	{
		交换$Q'_k$和$Q'_j$内的$ID $组份\;
		找到最小的 $\Delta' $ 并且 
		重置 $Q'_k,D'[k]$\;
	}
	\If{$\Delta' \leq \Delta $}
	{
		\textbf{return}  $\left\{ Q^{'}_{k},D'\left[ k\right] ,k\in \left[ 1,K\right] \right\} $\;
		
	}\Else{
		\textbf{return}  $\left\{ Q_{k},D\left[ k\right] ,k\in \left[ 1,k\right] \right\} $\;
	}
}
\end{algorithm}

\begin{figure}[!ht]
	\centering 
	\vspace{-1.5mm} 
	\includegraphics[scale=1]{asgen.pdf}
	\caption{PL端并行处理流水线一般模型} \label{fig:asgen}
\end{figure}

下面对本文提出的并行查找块结构进行一般化描述，如图。并行结构查找块是PL中承载用户定义需求的核心逻辑框架，一种可扩展形态如图\ref{fig:asgen}所示。总体看系统中一共有$m(m\geq 1)$个处理流水线，每个流水线为一条完整的并行查找块结构。在每个流水线中总共有$ n(m \geq 1) $级块结构。图中红色正方形框圈出的是流水线中的并行查找结构（基本处理单元，Basic Processing Unit，BPU）。在每一个BPU中，都有并行的“处理引擎”，本文中将其抽象为“数据-执行”（Data-Action）模型，其可以为查表结构，也可以为自定义计算结构。每一个BPU的输入（数据流）个数与执行引擎个数相同，但BPU对外的输入输出个数不一定相同。假设第$i$级BPU的输入有$K_i$个，输出有$K_{i+1}$个。在第$i$级中，流调度模块（流量均衡查找模块与交叉开关）将$K_i$个输入数据流尽可能均衡地分发到$K_{i+1}$个执行引擎中。执行模块的设计以及系统参数（$m,n,K_i$等）可由高层次综合工具，SDNet或HDL语言来定制。每个流水线之间是完全并行的，并无数据交叉，每个流水线内部存在数据流量交叉。因此在计算流表与流量分配问题时，可利用算法\ref{lbbtcinit} 、算法\ref{lbbtcms} 针对每一条流水线独立计算。




\BiSection{系统开发以及测试}{System Performance and Evaluation}\label{spe4}

对前文提出的系统进行评估，本文完成了4000+行代码在基于Xilinx ZC706开发板上对“自适应交换”系统做了原型实现。完成内容包括了SS功能，以及PL端的三个用户定义测试例子。在SS中，本文将一个发包模块集成在“自适应交换”系统中，以方便大规模地进行流量测试。本文选用了5种流量激励：其中两条真实流量（1）由ISP骨干网捕获（WIND流量），（2）基于基站通信的捕获流量（LTE流量），三条模拟流量（3）服从指数分布流量，（4）Pareto分布流量以及（5）平均分布流量：

\begin{enumerate}
	\item WIND真实流量。此流量由位于新西兰的接口速率3.7Gbps的ISP路由端口捕获而成。流量包括了4十亿个数据包，持续时间60分钟。
	\item LTE真实流量。此流量捕获于校园内的移动信号运营商4G基站，捕获数据时保护用户隐私，隐去了数据包数据信息，只保留了包头信息和包长度信息。在回放数据时，本文使用随机数字填充数据包内容。此流量共计1亿个数据包。
	\item 指数分布流量。流量中每条流的大小服从指数分布，指数分布是一种长尾分布，小流数目众多。本文将指数分布中的最大流量设置为100Mbps，尺度参数$\lambda=30$。指数分布流量平均流大小为3.33MB。
	\item Pareto分布流量。流量中每条流的大小服从Pareto分布，Pareto分布是一种厚尾分布，形状参数$\alpha=30$，位置参数$\beta=1.05$。本文设置最大单流流量为100Mbps，平均流大小为8.6MB。
	\item 平均分布流量。流量大小服从1KB到100MB的平均分布。平均流量大小为50MB。
\end{enumerate}

本文通过300+行python代码实现了PL映射优化算法，其运行在通用服务器中（Dell R620主机，8G内存，2.80GHz主频，Ubuntu16.04）。本文利用PyPy工具链加速了python程序性能优化。

为提现“自适应交换”系统的灵活性，本文实现了三个网络功能应用场景。

\BiSubsection{拥塞控制}{Congestion Control}\label{chap471}

NDP\citeup{handley2017re}可保障在交换机内拥塞丢包时“小批量”数据包的转发延迟。当数据平面拥塞时，通路缓存空间占用率会上升。对于短会话数据包，等待每一个阶段的缓存排队将导致会话完成时间成倍增长。此外，当数据平面内发生丢包现象，按目前的TCP协议将使用很长时间才能被动发现，导致重传时间过久，通信时延成倍增长。NDP则是一种数据平面内的拥塞控制机制，可在数据平面内实时发现丢包事件，并快速双向通报（向源与目的同时通报），实现快速重传，与快速窗口收敛。NDP是一类“事件驱动”处理算法，在通用可编程数据平面内只支持包触发行为，因此无法直接应用。为在“自适应交换”系统中实现NDP，本文利用MMU在SS端实现逻辑输出队列，在PL端实现了两个NDP执行集：（1）检测队列深度是否到达拥塞域值；（2）发送精确信息控制来调整包大小。前端编译器将此执行集编译到BPU和流水线内，同时实现功能落地和性能扩展。

\BiSubsection{网络测量}{Network Measurement}\label{chap472}

前文介绍DISCO\citeup{hu2013discount}是一种高效率高精度的网络流量测量算法。前文已经在基于网卡端的可编程硬件中详细阐述了DISCO的硬件优化方法。由于目前商用的可编程数据平面（P4/非P4交换平台）同样缺乏计算指令集以及灵活的判断分支方法，DISCO依然难以使用目前技术在交换核心节点实现。在“自适应交换”系统中，本文将实现交换机内DISCO硬核。传送进入PL端的元祖数据（metadata）须包含，包长度，流标识号（flow ID）。PL端将流统计结果暂存于板载高速内存。

\BiSubsection{有状态防火墙}{A Stateful Firewall}\label{chap473}

本文利用PL的可扩展性在“自适应交换”系统中实现了一套有状态防火墙转发引擎，目前有状态转发在商用可编程交换机内依然无法\citeup{sun2017sdpa,kohler2018p4cep}实现。状态引擎为待过滤流保存连接状态。须实现一个组合表，由两个子表构成：（1）基本的查找表实现“匹配-执行”操作；（2）存储一系列状态转移图的信息。当数据包到达组合表后，执行器依据当前状态标志以及匹配结果对数据包进行下一步处理。之后根据下一状态查找表更新基本表以及目前状态标志位。

\BiSubsection{资源消耗}{Resource Consumption}\label{chap474}

\begin{table}[!ht]
	\renewcommand{\arraystretch}{1.2}
	\centering\wuhao
	\caption{各组件性能以及处理时延} \label{table:as1} \vspace{2mm}
	\begin{tabularx}{\textwidth}{c|ccccc}
		\toprule[1.5pt]
		\multicolumn{2}{c}{组件} & 最大运行频率 / MHz & 阻塞时间 / 周期 & 时延 / 周期 & 吞吐 / Gbps \\
		\midrule[1pt]
		\multicolumn{2}{c}{MMU(1536$ \times $4096字节)} & $235$ & 3 & 2 & 147 \\
		\multicolumn{2}{c}{L3包头解析} & $400$ & 1 & 6 & 125 \\
		\multicolumn{2}{c}{分类器\&256查找子表} & $400$ & 1 & 5 & 125 \\
		\multicolumn{2}{c}{4:4交叉开关} & $303$ & 1 & 2 & 378 \\
%		\cline{1-6}
		\multirow{3}*{匹配表}& CAM 4k 64bit & $176$ & 1 & 3 & 110 \\
		& LPM 4k 32bit& 191& 1& 20& 119 \\
		& TCAM 1k 32bit& 171& 1& 22& 107 \\
		\cline{1-6}
		\multirow{3}*{实用场景}& NDP & $312$& 1 & 8 & 390 \\
		& 测量& $265$ & 1 & 3 & 165 \\
		& 防火墙& $416$ & 3 & 7 & 92 \\

		\bottomrule[1.5pt]
	\end{tabularx}
\end{table}

\begin{table}[!ht]
	\renewcommand{\arraystretch}{1.2}
	\centering\wuhao
	\caption{各组件FPGA内资源消耗情况以及所占比例} \label{table:as2} \vspace{2mm}
	\begin{tabularx}{\textwidth}{c|cccccc}
		\toprule[1.5pt]
		\multicolumn{2}{c}{组件} & LUT & LUT RAM  & FF  & BRAM & DSP \\
		\midrule[1pt]
		\multicolumn{2}{c}{MMU(1536$ \times $4096字节)} & 626/0.29\% & 272/0.39\% & 212/0.05\% & 7.5/1.38\% & 0\\
		\multicolumn{2}{c}{L3包头解析}                  & 242/0.11\% & 16/0.02\% & 862/0.20\% & 0 & 0\\
		\multicolumn{2}{c}{分类器 \& 256查找子表}          & 40/0.02\% & 16/0.02\% & 36/0.01\% & 0.5/0.09\% & 0\\
		\multicolumn{2}{c}{4:4交叉开关}                 & 252/0.12\% & 88/0.12\% & 298/0.07\% & 1/0.18\% & 0\\
		%		\cline{1-6}
		\multirow{3}*{匹配表}& CAM 4k 64bit             & 760/0.35\% & 1937/2.80\% & 1693/0.39\% & 15/2.8\% & 0\\
		& LPM 4k 32bit                                  & 861/0.39\% & 813/1.2\% & 2466/0.57\% & 23/4.2\% & 0\\
		& TCAM 1k 32bit                                & 36434/17\% & 23022/33\% & 35068/8.1\% & 3/0.55\% & 0\\
		\cline{1-7}
		\multirow{3}*{实用场景}& NDP                    & 271/0.12\% & 0 & 8/0.01\% & 7/1.28\% & 0\\
		& 测量                                         & 933/0.43\% & 20/0.03\% & 599/0.14\% & 8/1.47\% & 1/0.11\%\\
		& 防火墙                                        & 37/0.02\% & 0 & 232/0.05\% & 8/1.47\% & 0\\
		
		\bottomrule[1.5pt]
	\end{tabularx}
\end{table}


表\ref{table:as1}和表\ref{table:as2}展示了“自适应交换”系统关键部件的FPGA实现特性以及资源消耗。本文记录了实现模块的最大运行频率，以及实现流水线中的阻塞情况和模块内部流水线时延，最后得出了理论状态下的最高吞吐性能。由于SS端须依赖ASIC实现，因此部分模块性能还会显著提升（如MMU将会有十到二十倍增长空间）。

\BiSubsection{控制平面计算量}{The Calculation in Control Plane}\label{chap475}

PL端系统高速运行依赖于流量的负载均衡，但实际流量依然存在平均速率值的上下偏离。因此实时系统须对流表分配进行重排。重排可重新使流量均衡，但过程中也会导致大量的控制信令占用控制通道带宽。因此系统在两种情况下会对流表分配进行重排：（1）用户增加或修改流表项；（2）等待一定时间$T(t)$之后。$T(t)$可根据流量分布定义，也需要考虑安全通道的带宽容量。

1）流表分配运行时间。

\begin{figure}[!ht]
	\centering 
	\vspace{-1.5mm} 
	\includegraphics[scale=0.96]{asper.pdf}
	\caption{流量均衡算法计算性能（时间）。在不同的均衡性（$\delta$=9\%,7\%,5\%,3\%）需求下为$K=8$条子流水线在不同流量下的计算时间分布} \label{fig:asper}
\end{figure}

\begin{figure}[!ht]
	\centering 
	\vspace{-1.5mm} 
	\includegraphics[scale=0.96]{ask.pdf}
	\caption{在不同子流表数目下（$K=$4,6,8），且$\delta$=3\%时的计算时间分布} \label{fig:ask}
\end{figure}

本文在普通服务器中运行控制平面启发式算法，为了验证算法实时性，本文统计未配置状态下的数据平面（无转发逻辑，无实时统计结果）得到配置的时间。系统以一种随机状态开始接收流量，并计算每个ID组份的流量大小，在一定时间段之后，控制器重新计算新的流量均衡的配列方式并重新下载入数据平面。实验使用前文提到的流量数据。本文以精度$\delta$来衡量流量均衡度，$\delta$定义为每个子流水线的流量值的算数平方差，因此$\delta$值越小，均衡性越好，也越能发挥系统的最大性能。

在系统启动须先定义$\delta$的可接受域值，本文设定$K=8$，$ J=256$，如图\ref{fig:asper}所示，当设定均衡度为5\%时，可保证在75\%的情况下计算时间小于0.01秒。作为对照，如图\ref{fig:ask}本文测试了当子流水线数目（$K$）不同时计算时间的分布。当并行度增大时，显然算法的收敛时间会增加，在本实验的三个测试功能中，超过90\%的计算时间都小于0.5秒，并且能够以100\%置信度保证计算时间小于1.12秒。

\begin{figure}[htbp]
	\centering 
	\vspace{-1.5mm}
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[scale=1]{astradeoff.pdf}
		\caption{控制信道消耗量与流量均衡性之间的折中，实验须使用真实流量，已获得数据包之间的时间} \label{fig:astradeoff}
	\end{minipage}
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[scale=1]{astput.pdf}
		\caption{BPU中不同并行度对总体性能的提升效果，FPGA主频200MHz} \label{fig:astput}
	\end{minipage}
\end{figure}



2)控制通道带宽消耗。

控制器判断流量均衡，并决定是否对其进行重新调整。本文通过对真实流量的测量，对在不同均衡需求以及不同控制通道带宽使用量进行测量。在实验中每使用一个控制数据包，也定义为增加一次流表修改次数。如图\ref{fig:astradeoff}所示，在真实流量（WIND流量）下，可以看出选择更新时间$ T(t) $=45秒或90秒，平均可以保证流量均衡程度在9\%以内。若每个控制信令包长度128Bytes，则总共消耗信道带宽不超过3KBps。





3）吞吐性能提升。



如图\ref{fig:astput}所示，画出了吞吐随处理引擎并行度的变化趋势。计算处理性能时，须依照运行频率，数据总线宽度，以及真实流量数据包的包长度（平均约650字节）。可看出处理吞吐基本与并行数量呈线性正比。三种应用在PL中的总共处理时延为0.136$ \mu s $，0.142$ \mu s $，0.130$ \mu s $（测量，防火墙，拥塞控制）。


\BiSection{本章小结}{Conclusion}\label{chap48}

本文为增强数据平面转发设备的网络随路计算能力，提出了一种自适应交换系统（Adaptive Switch Architecture, AS）。AS的核心思想是保留ASIC交换芯片中的交换网络结构以确保高性能，但将网络数据可编程性卸载到FPGA可编程硬件中。本文在AS架构中实现了三种难以在目前交换系统中实现的例子，集中体现了AS架构可为目前的交换芯片带来极高的灵活性。此外，本文设计了资源消耗可控的多级并行流水线，并使以上三个应用达到了数个Tbps的吞吐性能。虽然PL（FPGA）端的接口吞吐性能依然无法与ASIC相比，但是我们不需要将完整的数据包送入PL，并且不是所有的数据流都需要被用户定义逻辑处理，因而可以极大缓解二者之间的性能鸿沟。综上，AS架构的性能已经达到了业界芯片标准水平，与其相比具有很高的竞争力。



