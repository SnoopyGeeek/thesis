% !Mode:: "TeX:UTF-8" 

\BiChapter{研究可编程设备加速网络硬件交换层方法}{Switch} %15页


\BiSection{本章引论}{aa}


\BiSection{问题背景}{aa}

可编程网络硬件可以运行传统部署在服务器内的功能，从而将网络性能提高数个量级。由于软件定义网络概念的提出，网络灵活性增强，运营商更倾向于自主管理网络内设备的所有行为。具有灵活性的管理方式可以大大增加网络的运行效率，为特定的功能定制专用的网络场景。  现有的面向网络通信的可编程数据平面一般有x86平台，开放接口配置的ASIC转发芯片，以及上一章本文提到的FPGA平台。他们在处理灵活性，转发性能等方面均有长足进步，但正如本文第二章中介绍，在面对转发容量进一步增强，灵活性需求进一步开放的网络应用时依然面临新的挑战：

1）软件具有高度灵活性，但处理性能低下。

数据包交换对于CPU架构平台来说是一种很低效的机械劳动。在软件层面，数据包转发算法已经被优化的极为高效，但面对无穷无尽的任务量，依靠CPU指令集的处理架构存在访存效率低、无法批处理等问题。CPU无法发挥自己实现灵活程序跳转、分支的优势，因此目前高性能软件网络处理性能也只能达到10Gbps（单核心）。对于现代服务器面向200G接口，汇聚层交换转发动辄5、6Tbps的性能需求是远远无法满足的。

2）基于FPGA可编程硬件平台。

FPGA是一种逻辑可编程芯片，可提供软件一般的灵活性也具有硬件的性能。目前，主要的云计算厂商阿里巴巴、亚马逊、微软都在数据通路内部署了FPGA加速引擎以同时满足性能和灵活性的需求，例如提升网络加解密性能，定制传输层协议等。然而由于FPGA电路编码转换采用“查找表+内部互联网络”的原理，为保证时间同步性，使得FPGA综合对外的处理主频只能达到200MHz，这样即使FPGA内流水线每个周期都能处理一个数据包，总吞吐性能也无法超过100Gbps\citeup{wang2017p4fpga,netfpga2014}。虽强于软件，但远差于网络的核心层性能需求。

3)协议无关概念的交换芯片。

与协议无关交换架构（Protocol-Independent Switch Architecture,PISA）相对应的是P4（Programming Protocol-Independent Packet Processor）。P4是一种专用的编程语言，其目标是为任意包协议提供一种基于ASIC硬件（PISA）的现场可重配置能力。根据本文在第二章的介绍，PISA有能力实现自定义包头解析，自定义流表的组成结构，并且最重要的它拥有最强大的处理能力（12.8Tbps，显然是唯一可以胜任核心转发设备的架构）。虽然这大大解决OpenFlow编程能力不足以及其设计本身所带来的的可扩展性差的难题，但它不具备真正意义上其所追求的“图灵完备”可编程。例如：（1）数据通路内的处理动作只能被数据包触发，而无法响应其他行为。针对一些QoS场景系统更希望根据队列深度做出一些响应，比如拥塞控制场景下的控制算法NDP\citeup{handley2017re}；（2）PISA缺乏计算、控制属性的指令集，例如乘除法、灵活分支判断等；（3）PISA为无状态转发的可编程流水线设计，则对状态协议处理以及有状态计算造成了困难（防火墙等）。

PISA转为通用型的无状态转发同时提高了性能与灵活性，本文将对PISA架构芯片的目标处理内容做进一步扩展。本文的目标是设计一种适用于网络交换层的交换机架构，这种架构可从计算平面高灵活性的工作任务分离和卸载到网络中，包含但不限于有状态转发、复杂计算以及自定义触发方式。



\BiSection{系统架构介绍}{aa} 

P4是目前广泛使用的可定义数据平面内的数据包解析和查表过程的语言。P4将流水线外的功能需求通过P4\_extern模块来让用户自行定义。然而P4\_extern的编程范围只在PISA提供的执行器序列之内，例如校验算法，位移，插入数据等。目前的PISA芯片架构对于“PISA\_extern”的功能需求没有支撑能力。一种显而易见的解决方案是当需要一种新的“PISA\_extern”时，重新设计一种支撑新特性执行器的ASIC硬件。但这与PISA高灵活性的设计初衷相矛盾，从经济实用性角度分析也是不合理的。

本文提出一种硬件异构型的交换机架构，可以支持任意P4\_extern所定义的功能。本文将这种结构命名为“自适应交换”，首先本文需要解决如何设计硬件结构使得灵活性和性能可以充分展现，其次本文需要解决如何开发编程语言以映射到自适应交换的数据平面。最终为了展示自适应交换的高扩展性，本文将上小节内提出的PISA难以应用的场景均在自适应交换平台中实现。

\BiSubsection{架构设计}{aa}

\begin{figure}[!ht]
	\centering 
	\includegraphics[scale=1]{asarch.pdf}
	\caption{自适应交换结构框图} \label{fig:asarch}
\end{figure}

自适应交换的硬件架构框图。自适应交换包括两部分，1）固定功能的ASIC交换系统（Switching System, SS）；2）用户定义可编程逻辑块（Programmable Logic, PL）。SS基于标准的交换芯片（switching ASCI）处理功能，PL利用FPGA类的可编程硬件支持用户自定义逻辑。一个标准的交换芯片功能包含了数据包头的解析，基本的ACL功能和各个字段的匹配以及执行转发/丢包等动作。额外还会包括流量管理、队列调度等QoS功能。架构中SS与PL两部分可由双芯片拼接的方式实现，SS部分利用一个标准传统的交换芯片（既可以支持P4编程，也可以不支持P4编程）构成。PL端使用FPGA芯片或由FPGA构成的多用途片上系统（MPSoC/ACAP）芯片。在双芯片拼接方案中，两个独立的芯片系统须通过高速互联总线相连接，例如PCIe接口，以太网接口或者类似的收发器。相应地自适应交换系统也可以由单独一颗芯片构建而成，PL与SS部分通过片上高速总线相接，例如，AXI协议总线。

如图\ref{fig:asarch}左半部分所示，交换网络是SS的核心组成部分。交换网络支持网络入端接口与出端接口多对多的数据包互联传送，一般由交叉开关（cross-bar）构成。数据包经由物理网络接口进入SS系统，在交换网络两边，数据包会经过入端（Ingress）处理流水线，以及出端（Egress）处理流水线。在两条流水线的组成分别有包头解析（抽取必要的包头域数据），流表（对包头域数据进行匹配和执行相应的动作集），封包（重组/修改数据包结构），以及流量管理（包缓存/调度/限速等）。

数据包进入自适应系统时，首先经由SS端的处理，通常情况下大部分数据包可以完整地被SS处理，并返回外部网络。只有在SS内的功能无法满足需求的那部分数据包会再次送入PL端做协同处理。对于送往PL端处理的数据包，在SS端在片上存储中（或使用片外存储）也保留有一份完整是数据包备份，SS只将数据包的元祖数据信息送给PL。元祖数据中包括了PL完成处理所需要的定制化的包头信息或者其他描述信息。PL处理完成元祖数据后，会更新重写元祖信息中的包头以及描述字段，再返回给SS。最后SS将备份的原始数据包与更新的元祖数据重新整合成一个完整的数据包后再次对其执行转发操作，或简单丢包。

本文将在传统的交换芯片中增加一个包存储管理机制（Memory Management Unit, MMU），图\ref{fig:asarch}中左边虚线内部所示。当数据包元祖信息送往PL处理时，这个数据包在SS端的完整备份由MMU管理。MMU应包含三个主要功能：（1）动态申请/释放数据包的存储块位置及其大小；（2）驱动数据包向内存中读写时序；（3）读出备份数据包后与从PL返回的新元数据对接重组。

上述自适应交换流程与原理是建立在本文对以下两方面的分析结果和假设之上。首先，在PL中处理的信息通常只依赖于包头，或者数据包包首部分的字段。在设计中被交换到PL的元数据可以被灵活地定义，并且保证只占用SS与PL之间一定量的通信带宽成本。在某些极端的例子中，如果处理过程需要数据包所有的数据时，元祖数据也将会包含一个完整的数据包内容。第二，不是所有的数据包处理都需求调用PL端的功能。否则，如果有一种功能是普遍适用的，则一定会集成在现行的通用交换芯片内。或者这种功能需求可以直接拆分出来一部分由SS端完成处理。

自适应交换的高性能来自于对网络通用数据包包长的分析。本文考虑到，目前网络内平均包长度为600字节左右，假设元祖数据包含整个包头部分（64字节之内），则SS与PL之间的通信成本只占需求流量的10\%。即，利用单插槽的PCIe4.0接口（256Gbps）作为高速互联，自适应交换系统可以将目前最高速度（12.8Tbps）交换芯片中20\%的数据（>2Tbps）卸载到用户硬件可编程逻辑中进行处理。这远远超过了（$\times$10倍）单纯由FPGA组成的可编程数据平面的处理性能，也是本文研究的最主要的动机。

\BiSubsection{开发流程}{aa}

对于基于FPGA的PL端，基本的开发设计流程包括如下几部分：

\begin{itemize}
	\item 定义数据包处理需求以及所需要的数据流模型。
	\item 编写处理规则对应的函数或流程代码。程序代码可包括高级语言如P4/P4\_extern、P4\_FPGA，或者底层的硬件描述语言如verilog HDL。此外包括数据平面高层次生成系统，如SDNet\citeup{sdnet}，也可以用于加速PL端的开发工作。
	\item PL端的机器码编译器。不同PL目标器件下的编程，往往有不同的编译步骤。对于基于FPGA的编译器，本文使用FPGA芯片厂商提供的编译环境。但本文最主要的贡献是解决如何将数据包流处理模型在异构形态下重新组织，并且完成高性能的异构形态下包处理逻辑无语义偏差的拆分和翻译。
\end{itemize}

\begin{figure}[!ht]
	\centering 
	\includegraphics[scale=1]{asdev1.pdf}
	\caption{基于SDNet工具链的FPGA-PL端编译流程} \label{fig:asdev}
\end{figure}

本文利用Xilinx SDNet/P4-SDNet作为建立本文设计原型系统的基本开发工具，基于FPGA的PL编译流程如图\ref{fig:asdev}所示。SDNet和P4-SDNet已经是成熟商用包装解决方案，并包含了为FPGA数据平面设计的P4语言到verilog硬件模块的编译工具链。P4-SDNet内建两种专为P4$ _{16} $（提取包头数据，修改包头域值）提供的P4\_extern功能。这种extern功能给用户提供了足够灵活的直接修改原始数据包头域的能力，但目前此编译器并不支持其他extern对象的编译，且用户也不可定义其自己的extern对象。

使用其类似的思路，本文在开发流程中增加新的编译模块，使其将能够使用户扩展extern对象定义的范围：扩展前端编译器，使其支持其他的高层次描述（描述中增加标记符以提高后端编译器的性能），并将其转译为逻辑中间表示层（IR）；最后通过后端编译器再次映射到PL端。
本文提出的硬件转发体系结构与通行产品不同，下面将详细介绍本文开发流程与经典可编程数据平面（以P4为例）之间的差别。

\begin{figure}[!ht]
	\centering 
	\includegraphics[scale=1]{ascomp.pdf}
	\caption{基于SDNet工具链的FPGA-PL端编译流程} \label{fig:ascomp}
\end{figure}

图\ref{fig:ascomp}中左边为经典的P4程序编译标准流程，对P4目标器件编程需要两个描述文件。首先是P4行为描述程序代码，包含了待处理数据包的包头域定义，包头域匹配关系图，流表设计，执行动作设计等。第二为P4架构图，此文件由设备厂商提供，编译器须根据不同底层芯片实现样式进行有针对性的编译，这样编译器可根据结构图对程序进行等效变换以及重组。前端编译器会将文本代码转换为逻辑关系表，也称作中间表示层（Intermediate Representation, IR），后端编译器一般有厂商提供，可将符合规范的IR表示直接转为机器二级制配置文件。远端控制器将配置文件下载入可编程交换机设备即完成了数据平面重配置过程。此时，控制器也需要收集P4定义信息，以方便后续添加流表项等网络功能设置。

根据之前本文提到的自适应交换系统的额外特点，如图\ref{fig:ascomp}右边所示，本文对基础编译框架进行补充。首先P4结构图需要修改为自适应交换机的底层实现，例如需要体现包头域协同处理机制。其次本文支持高层次语言的PISA\_extern对象编程，因此在额外扩展的前端编译器中，需要将高层次编译器适配入本流程。新增部分由红色区域块所示。最终后端编译器配合P4结构图生成FPGA中的RTL代码，交由FPGA厂商工具链生成二进制流文件（bitfile），通过板上运行时代理下载入FPGA即完成了PL端配置。SS端配置同经典过程所述。

\BiSubsection{高层次语言映射样例}{aa}



{\fontsize{10pt}{0.5\baselineskip}\selectfont
	\begin{lstlisting}[caption={数据平面接口：PL端高层语言描述},label={ascode1}]
	extern extern_example{           				//声明模块名称：extern_example
		ext_type(in bit<wdth> input_Port);			//声明接口位宽以及数据流方向
		int a = VHDL_method_example();				//模块使用其他语言快verilog
		void CPP_HLS_method_example(a);				//模块使用类C的函数描述语言
	}
	control control_example{						//实例化样例模块
		extern_example(0x0) my_extern_example;
		action my_extern_call(){					//调用PL端功能
			my_extern_example.C_HLS_method_example();
		}
	}
	\end{lstlisting}
}

PL以FPGA芯片为例，自适应交换平台可以将SS交换芯片无法完成的用户定义功能映射到FPGA中。用户的额外功能对象须遵循PL端输入输出接口规范。P4语言风格的数据平面接口规范如代码 \ref{ascode1} 所示。代码定义了一个名为“extern\_example”的PL模块，模块可由C语言或HDL风格实现。对外接口位宽以及方向需要标记。在后期可实例化一个或多个样例，每个样例可自由调用PL端模块中的任意描述功能。

{\fontsize{10pt}{0.5\baselineskip}\selectfont
	\begin{lstlisting}[caption={extern实例的C++类声明},label={ascode2}]
	#include <hls/hls_sim/extern.h>					//头文件extern.h 定义了外围数据传输
	using namespace std;
	template <typename .. Args>
	using ActionPrimitive =
		hls :: ActionPrimitive <Args ...>;
	using hls :: Data;
	class extern_example: public ExternType{
		public:
			void init() override{}
			void C_HLS_method_example(){...}
	};
	HLS_REGISTER_EXTERN(extern_example);
	int import_extern_example(){return 0;}
	\end{lstlisting}
}

如代码 \ref{ascode2} 展示了C++风格extern函数。头文件“extern.h”中定义了核心函数之前的流水线结构（需要用户注意并修改），无需提现在核心描述代码中。FPGA高层次编译工具链（SDx，Bluespec System Verilog）可读取此类文件，并综合成带有用户功能的IP核，小的IP核可便于集成进更大的FPGA工程。

{\fontsize{10pt}{0.5\baselineskip}\selectfont
	\begin{lstlisting}[caption={extern实例的VHDL模块声明},label={ascode3}]
	`timescale 1ns/1ps
	module VHDL_method_example#(
		parameter DATA_WIDTH = D_WIDTH,
		parameter CTRL_WIDTH = C_WIDTH,
		parameter EXTERN_REG_WIDTH = R_WIDTH,
		...
		)(//---data interface
			input		[DATA_WIDTH - 1:0]	in_data,
		  //---register interface
		  	input [EXTERN_REG_WIDTH - 1:0]  in_reg,
		  //---misc
		  	input							clk,
		  	input							reset);
		  	
		//local parameter
		//wires/regs
		//modules
		//method example logic
	endmodule //VHDL_method_example
	\end{lstlisting}
}

代码 \ref{ascode3}展示了VHDL语言实现PL实例模块，在VHDL中须描述数据总线位宽，寄存器通路接口等。对于其他硬件系统，用户只需要指定通信用同步数据总线的时钟周期（时间约束）。
实际上，本文所描述的所有编译工具链仍处于设计阶段，并需要大量代码工作完成自动化处理。本文主要目的在于规划框架以及作为商用工具链的补充设计参考。硬件开发时间周期过长，为快速进行数据平面验证，本文生成的所有bit文件均为手动整合代码。

\BiSection{硬件设计}{aa}

\BiSubsection{协议设计}{aa}

\begin{figure}[!ht]
	\centering 
	\includegraphics[scale=1]{asframe.pdf}
	\caption{SS与PL内部通信帧格式} \label{fig:asframe}
\end{figure}

上节提到自适应交换架构中SS端与PL端的通信须包含元祖数据以及包头，本节论文详细讨论通信帧格式。本文设计了一种通信帧（可满足大部分需求，用户也可以自行定义），格式如图\ref{fig:asframe}所示。首先帧头指定了帧的编号PID，帧由SS中的数据包生成，完整的数据包与帧使用同一个PID编号，方便PL协处理器处理之后重新组合。元祖信息（metadata）包括了数据包转发过程中的信息，例如输出端口，组表编号，丢包等。用户元组信息为帧有效字长，由于用户可以定义帧长度，这将给用户极大的灵活性空间。此外用户还可定义包头长度以及携带的数据包负载信息长度等。SS与PL之间传递的大部分操作数都是商用ASIC中已经实现的指令，因此SS端设计可以极大程度复用已有芯片设计。须增加的功能只有帧长度信息等域，这样可使对ASIC改动降到最低。

\BiSubsection{SS端固定功能}{aa}

SS端对于目前商用普通交换芯片中需增加的功能为第一小节中提到的图 \ref{fig:asarch}内左边虚线内的MMU模块。自适应交换的核心思想是利用基于ASIC的交换系统（SS）提供包交换的高性能，而用可编程硬件协处理器（PL）提供包处理的灵活性。因此在SS端本文将包处理逻辑做的最简化，只保留一个用户可配置逻辑（即帧长度的定义）。MMU将用户定义的不超过帧长度的包头数据（指某一区间段）复制到通信帧中即可。MMU的其余工作还包括了动态申请和释放数据包缓存空间，以及重组数据包。接下来本小结主要介绍MMU的分析与设计。

假设SS与PL之间单向通信带宽有256G(PCIe4.0)，以帧内包含完整64字包头节为例，则包速率达到380Mpps。目前交换芯片的时钟驱动频率大约在1GHz到1.5GHz之间。对于MMU模块，若留给每个数据包的处理时间3周期则不会超过时钟频率（需要约1.2GHz主频）。在之前的MMU设计中，内存被分割为每页256字节。调度器以页为单位进行分配，维护逻辑映射与物理映射之间的关系由于页容量比较小而变得比较复杂。因此MMU瓶颈处理一个分配任务需要耗时25周期\citeup{wang2017p4fpga}，尽管这种设计思路在资源利用率上比较占优（平均资源浪费仅为1/2page），但它的时间复杂度太高，导致分配性能受影响。在ASIC的存储设计中，其经济性要好于由FPGA组成的片上存储资源。因而本文在设计存储单位时将每个地址对应的容量配额扩大为1.6KB（可完整放下一个最大包）。在分配以及查找长包时，可节省逻辑地址与物理地址之间多次正向/反向映射过程，因此大大提升分配速度。

\begin{figure}[!ht]
	\centering 
	\includegraphics[scale=1]{asmmu.pdf}
	\caption{高pps性能MMU的实现方案} \label{fig:asmmu}
\end{figure}

如图\ref{fig:asmmu}所示，本文使用了一组“标志位”寄存器来指示其对应的page空间是否被占用。当数据包请求一个新的可用地址时，优先译码器会将标志位最左边为“1”的位置作为地址（address）返回给调度器，且便立即将此位置的标志“1”更新为“0”。之后调度器会把数据包向此位置写入，并且令此位置的地址作为PID值填入PL通信帧的首部。当MMU收到一个含有PID值的读请求时，MMU会读取以PID值为位置的存储空间并将其组合为数据包重新发出到入队流水线中，完成后立即将此位置的标志位为“1”。此时数据包的metadata中一般包含了由PL端处理完成给出的执行需求，SS端无需对此包做额外的分析工作。为了防止地址冲突以及标记位数据不同步带来的读写错误，优先译码器必须在完成上一个“读/写”请求之前，更新空闲地址，以备调度器无缝衔接下一个写请求，读请求则无需使用优先译码器结果，无操作风险。上文分析过，对每个读写操作，MMU都预留了3个时钟周期的处理时间。在系统运行后，由于数据包连续到达以及连续读写，因此在一段时间内，平均读次数是等于写次数的。MMU将读写交替进行，可为写请求争取6个时钟周期的译码时间，一般来讲对于有4k个位置的优先译码器，6个时钟周期足够通过硬件并行地两两比较找出最优位置（每周期比较两次的情况下$2^{6\times2}=4096$，但一般可以比较次数更多）。

接下来分析MMU应维护的内存容量大小。在元祖信息帧由PL返回之前，数据包都需要维持在MMU的存储空间内。假设信息帧处理时间为$T=n$(（微秒），上文提到总体包处理频率不超过$v=$380Mpps，因而平均在等待的数据包个数为$T \times v \leq =380n$。目前板间硬件处理时延应保证在3微秒以下，如果使用片内AXI总线互联，则完整处理时间应保证在1微秒\citeup{xilinxpcie}以内\footnote{在数据包的处理过程中查找TCAM表是最费时间的，一般需要20个时钟周期，但正常处理流程内TCAM表项不会超过2个。普通精确匹配查找表只需要不超过3个时钟周期即可。我们假设比较悲观的情形，PL内的处理过程包含5级TCAM查表以及10级精确匹配查表，则总共须耗费650时钟周期。若以200MHz主频运行FPGA，则总耗时约0.7微秒（一般只需要0.1微秒）。因此本文认为，PL的处理时延主要由信息帧传输过程贡献。PCIe收发器结构复杂，突发的批次传输时延较大（100时钟周期\citeup{xilinxpcie}），但高性能片上AXI总线的传输时延也很低（不超过10时钟周期）}。为保险起见，本文将处理时间余量增大十倍假设最大不超过10微秒，则根据上式，可约估SS端等待数据包个数为4k个。上文提到每个数据包存储位置占用1.6KB，则板上共计须设置最多为6.4MB存储空间。以目前的半导体工业水平可满足此需求。



\BiSubsection{PL设计}{aa}

基于FPGA的PL设计最大的挑战来自于大批量信息帧的处理速度（380Mpps）。对于PL内部逻辑，由于帧内包含了基本的metadata信息以及包头域段落，对PL的性能设计可将其按照网络处理最小包（64字节）的方式分析。按照FPGA主频200MHz计算，即使每个时钟周期都能由流水线处理完成一个数据包，FPGA的性能也只能达到200Mpps。对于某些带状态阻塞性处理机制（如有状态查找表），平均需要2-3个时钟周期才可处理完成一个数据包，因此FPGA的处理性能会下降到60Mpps。远远达不到系统设计的需求。

一种思路是将FPGA内一条流水线完整复制多份，只要保证每一条并行流水线流量均衡，且不超过70Mpps性能要求即可。但由于数据包流水线内有查表等大资源消耗量组件（1k深度的TCAM表项可占据整个FPGA逻辑资源的1/3），有限的FPGA逻辑面积不可能满足多条并行流水线的资源占用需求。

为解决上述挑战，本文提出一种微结构并行数据包处理流水线设计（micro-level parallel processing）。微结构流水线的核心思想是将原有的大表项拆分为多个小表项的等价组合，即通过不完整的复制大资源模块，而满足原始用户语义下的数据包处理需求。总结如下，本文将通过以下两个关键技术解决高效PL处理设计，1）提出一种资源高效的并行数据包处理架构，2）通过流表编译算法将流表项映射到多路子流表中，以达到高效利用硬件资源。

1）资源与性能

2）并行流水

%各种花哨 映射方法，加速方法
%
%rmt的crossbar介绍
%
%有状态转发

\BiSection{软件设计}{aa}

NP难题


\BiSection{系统性能评价}{aa}





\BiSection{本章小结}{aa}





